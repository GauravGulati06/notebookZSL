{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":896930,"sourceType":"datasetVersion","datasetId":479748}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:51:15.215632Z","iopub.execute_input":"2024-12-12T11:51:15.215971Z","iopub.status.idle":"2024-12-12T11:51:15.553665Z","shell.execute_reply.started":"2024-12-12T11:51:15.215933Z","shell.execute_reply":"2024-12-12T11:51:15.552743Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/boolq-dataset/dev.jsonl\n/kaggle/input/boolq-dataset/test.jsonl\n/kaggle/input/boolq-dataset/train.jsonl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:51:15.554632Z","iopub.execute_input":"2024-12-12T11:51:15.554994Z","iopub.status.idle":"2024-12-12T11:51:15.711782Z","shell.execute_reply.started":"2024-12-12T11:51:15.554966Z","shell.execute_reply":"2024-12-12T11:51:15.710821Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nfrom torch.utils.data import Dataset\n\nclass BoolQDataset(Dataset):\n    def __init__(self, base_dir, split):\n        \"\"\"\n        Args:\n            base_dir (str): Path to the base folder containing dataset splits.\n            split (str): Dataset split to use ('train', 'test', or 'dev').\n        \"\"\"\n        self.data_path = os.path.join(base_dir, f\"{split}.jsonl\")\n        if not os.path.exists(self.data_path):\n            raise FileNotFoundError(f\"Dataset split file not found: {self.data_path}\")\n        self.data = self._load_data()\n\n    def _load_data(self):\n        \"\"\"Loads data from the JSONL file.\"\"\"\n        data = []\n        with open(self.data_path, 'r') as f:\n            for idx, line in enumerate(f):  # Add idx while reading\n                sample = json.loads(line)\n                question = sample.get('question', None)\n                passage = sample.get('passage', None)\n                label = sample.get('answer', None)\n                data.append({\n                    'idx': idx,        # Add idx directly in the data\n                    'question': question,\n                    'passage': passage,\n                    'label': label\n                })\n        return data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            return [self._get_item(i) for i in range(*idx.indices(len(self)))]\n        \n        if isinstance(idx, int):\n            if idx < 0 or idx >= len(self.data):\n                raise IndexError(f\"Index {idx} is out of range.\")\n        \n        return self._get_item(idx)\n\n    def _get_item(self, idx):\n        sample = self.data[idx]\n        return sample  # No need to extract 'idx' here since it's already part of the data\n\nimport random\n\nclass DatasetWrapper:\n\n    def __init__(self, dataset_tag, base_dir, split):\n        if dataset_tag == \"boolq\":\n            self.dataset = BoolQDataset(\n                base_dir=base_dir,\n                split=split\n            )\n\n        elif dataset_tag == \"gsm8k\":\n            self.dataset = GSM8KDataset(\n                base_dir=base_dir,\n                split=split\n            )\n            \n        else:\n            raise ValueError(f\"Unsupported dataset_tag: {dataset_tag}\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def get_dataset(self):\n        return self.dataset\n    \n    def get_random_samples(self, num_samples, seed=None):\n        \"\"\"Get a list of random samples from the dataset.\"\"\"\n        if seed is not None:\n            random.seed(seed)\n            \n        num_samples = min(num_samples, len(self.dataset))\n        indices = random.sample(range(len(self.dataset)), num_samples)\n        return [self.dataset[i] for i in indices]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:51:15.713274Z","iopub.execute_input":"2024-12-12T11:51:15.713586Z","iopub.status.idle":"2024-12-12T11:51:17.225703Z","shell.execute_reply.started":"2024-12-12T11:51:15.713558Z","shell.execute_reply":"2024-12-12T11:51:17.224790Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\nmodels = {\n    \"qwen2.5_1.5b\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n    \"llama3.2_3b\": \"meta-llama/Llama-3.2-3B-Instruct\",\n}\n\n\nclass ModelWrapper:\n    \n    def __init__(self, model_name,secret_value_0):\n\n        # Hyperparams\n        self.max_new_tokens = 20\n        self.temperature = 0.001\n\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_auth_token=secret_value_0)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, use_auth_token=secret_value_0).to(device)\n\n        self.pipe = pipeline(\n            task=\"text-generation\",\n            device=device,\n            model=self.model,\n            tokenizer=self.tokenize\n        )\n    \n    def generate(self, messages):\n        \"\"\"\n        messages format:\n        [\n            {\"role\": \"user\", \"content\": \"Who are you?\"},\n            ...\n        ]\n        \"\"\"\n        output = self.pipe(\n            messages,\n            max_new_tokens=self.max_new_tokens,\n            temperature=self.temperature\n        )\n\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:52:44.500216Z","iopub.execute_input":"2024-12-12T11:52:44.500670Z","iopub.status.idle":"2024-12-12T11:52:44.507373Z","shell.execute_reply.started":"2024-12-12T11:52:44.500636Z","shell.execute_reply":"2024-12-12T11:52:44.506520Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"OUTPUT_FILENAME = \"boolqllama\"\ndataset = DatasetWrapper(\"boolq\", \"/kaggle/input/boolq-dataset\", \"train\").get_dataset()\nmodel = ModelWrapper(models[\"llama3.2_3b\"],secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:52:50.495675Z","iopub.execute_input":"2024-12-12T11:52:50.496032Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7bf7e2d58f402a98634fc3fdc952e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5043b61e88c549eb8fbabf864148c6ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10cc12dca0f04ca3aa7c4227f180fe4a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41416ad216594196bbaf7cc3046a7a3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28334445b29949608d1658e1597c9edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9344655e34bc4962966d157628090467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6578e3e2f46c475eb93b59f31a2fc95f"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import csv\nfrom tqdm import tqdm\n\ncsvfile = open(f\"{OUTPUT_FILENAME}.csv\", \"w\", newline='')\ncsv_writer = csv.writer(csvfile, delimiter=\",\")\n\ncolumns = [\"idx\", \"output\"]\ncsv_writer.writerow(columns)\n\nfor data in tqdm(dataset[:]):\n\n    question = data[\"question\"]\n    passage = data[\"passage\"]\n\n    prompt = (\n        \"You are given the following context:\\n\"\n        f\"\\n{passage}\\n\"\n        \"Answer the given question as only 'true' or 'false':\\n\"\n        f\"{question}\\n\"\n    )\n\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n\n    output = model.generate(messages)\n    extracted_output = output[0][\"generated_text\"][-1][\"content\"]\n    \n    csv_writer.writerow([data[\"idx\"], extracted_output])\n    if data[\"idx\"]%100 == 0:\n        print(extracted_output)\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:51:21.955800Z","iopub.status.idle":"2024-12-12T11:51:21.956096Z","shell.execute_reply.started":"2024-12-12T11:51:21.955956Z","shell.execute_reply":"2024-12-12T11:51:21.955971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}