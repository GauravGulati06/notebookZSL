{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":896930,"sourceType":"datasetVersion","datasetId":479748}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom torch.utils.data import Dataset\n\nclass BoolQDataset(Dataset):\n    def __init__(self, base_dir, split):\n        \"\"\"\n        Args:\n            base_dir (str): Path to the base folder containing dataset splits.\n            split (str): Dataset split to use ('train', 'test', or 'dev').\n        \"\"\"\n        self.data_path = os.path.join(base_dir, f\"{split}.jsonl\")\n        if not os.path.exists(self.data_path):\n            raise FileNotFoundError(f\"Dataset split file not found: {self.data_path}\")\n        self.data = self._load_data()\n\n    def _load_data(self):\n        \"\"\"Loads data from the JSONL file.\"\"\"\n        data = []\n        with open(self.data_path, 'r') as f:\n            for idx, line in enumerate(f):  # Add idx while reading\n                sample = json.loads(line)\n                question = sample.get('question', None)\n                passage = sample.get('passage', None)\n                label = sample.get('answer', None)\n                data.append({\n                    'idx': idx,        # Add idx directly in the data\n                    'question': question,\n                    'passage': passage,\n                    'label': label\n                })\n        return data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            return [self._get_item(i) for i in range(*idx.indices(len(self)))]\n        \n        if isinstance(idx, int):\n            if idx < 0 or idx >= len(self.data):\n                raise IndexError(f\"Index {idx} is out of range.\")\n        \n        return self._get_item(idx)\n\n    def _get_item(self, idx):\n        sample = self.data[idx]\n        return sample  # No need to extract 'idx' here since it's already part of the data\n\nimport random\n\nclass DatasetWrapper:\n\n    def __init__(self, dataset_tag, base_dir, split):\n        if dataset_tag == \"boolq\":\n            self.dataset = BoolQDataset(\n                base_dir=base_dir,\n                split=split\n            )\n\n        elif dataset_tag == \"gsm8k\":\n            self.dataset = GSM8KDataset(\n                base_dir=base_dir,\n                split=split\n            )\n            \n        else:\n            raise ValueError(f\"Unsupported dataset_tag: {dataset_tag}\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def get_dataset(self):\n        return self.dataset\n    \n    def get_random_samples(self, num_samples, seed=None):\n        \"\"\"Get a list of random samples from the dataset.\"\"\"\n        if seed is not None:\n            random.seed(seed)\n            \n        num_samples = min(num_samples, len(self.dataset))\n        indices = random.sample(range(len(self.dataset)), num_samples)\n        return [self.dataset[i] for i in indices]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:45:03.733583Z","iopub.execute_input":"2024-12-12T10:45:03.734988Z","iopub.status.idle":"2024-12-12T10:45:09.027759Z","shell.execute_reply.started":"2024-12-12T10:45:03.734947Z","shell.execute_reply":"2024-12-12T10:45:09.026758Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\nmodels = {\n    \"qwen2.5_1.5b\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n    \"llama3.2_3b\": \"meta-llama/Llama-3.2-3B-Instruct\",\n}\n\n\nclass ModelWrapper:\n    \n    def __init__(self, model_name):\n\n        # Hyperparams\n        self.max_new_tokens = 20\n        self.temperature = 0.001\n\n        self.model_name = model_name\n        self.pipe = pipeline(\n            task=\"text-generation\",\n            model=model_name,\n            device_map=\"auto\"\n        )\n    \n    def generate(self, messages):\n        \"\"\"\n        messages format:\n        [\n            {\"role\": \"user\", \"content\": \"Who are you?\"},\n            ...\n        ]\n        \"\"\"\n        output = self.pipe(\n            messages,\n            max_new_tokens=self.max_new_tokens,\n            temperature=self.temperature\n        )\n\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:45:09.029793Z","iopub.execute_input":"2024-12-12T10:45:09.030279Z","iopub.status.idle":"2024-12-12T10:45:31.874213Z","shell.execute_reply.started":"2024-12-12T10:45:09.030236Z","shell.execute_reply":"2024-12-12T10:45:31.873504Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"OUTPUT_FILENAME = \"rename\"\ndataset = DatasetWrapper(\"boolq\", \"/kaggle/input/boolq-dataset\", \"dev\").get_dataset()\nmodel = ModelWrapper(models[\"qwen2.5_1.5b\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:46:12.309313Z","iopub.execute_input":"2024-12-12T10:46:12.309718Z","iopub.status.idle":"2024-12-12T10:47:34.368691Z","shell.execute_reply.started":"2024-12-12T10:46:12.309681Z","shell.execute_reply":"2024-12-12T10:47:34.368017Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e29bf3a80b7143abaa0e6bf68c0fd625"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b293f108afa549fa8eff33299562b7ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9555ca6391e645bc8a85f8f92da30e6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21ed3d33f634e78a289bd72c818aac8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ec4b4ad597450d8c39aec2b0837c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3509589e0a5418db2e111b0c2631d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd26a50172b4f408b6e3115fc367786"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import csv\nfrom tqdm import tqdm\n\ncsvfile = open(f\"{OUTPUT_FILE}.csv\", \"w\", newline='')\ncsv_writer = csv.writer(csvfile, delimiter=\",\")\n\ncolumns = [\"idx\", \"output\"]\ncsv_writer.writerow(columns)\n\nfor data in tqdm(dataset[:]):\n\n    question = data[\"question\"]\n    passage = data[\"passage\"]\n\n    prompt = (\n        \"You are given the following context:\\n\"\n        f\"\\n{passage}\\n\"\n        \"Answer the given question as only 'true' or 'false':\\n\"\n        f\"{question}\\n\"\n    )\n\n    messages = [\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n\n    output = model.generate(messages)\n    extracted_output = output[0][\"generated_text\"][-1][\"content\"]\n    \n    csv_writer.writerow([data[\"idx\"], extracted_output])\n    if idx%100 == 0:\n        print(extracted_output)\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}